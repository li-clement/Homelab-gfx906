{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de747b9a-f19a-4284-88c1-8c163a053fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” æ£€æŸ¥å½“å‰æ˜¾å¡å ç”¨æƒ…å†µ:\n",
      "\n",
      "\n",
      "============================ ROCm System Management Interface ============================\n",
      "===================================== KFD Processes ======================================\n",
      "KFD process information:\n",
      "PID \tPROCESS NAME\tGPU(s)\tVRAM USED\tSDMA USED\tCU OCCUPANCY\t\n",
      "3021\tUNKNOWN     \t0     \t0        \t0        \t0           \t\n",
      "==========================================================================================\n",
      "================================== End of ROCm SMI Log ===================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 1. æŸ¥çœ‹å½“å‰å ç”¨æ˜¾å¡çš„è¿›ç¨‹ (å¦‚æœæœ‰ rocm-smi)\n",
    "print(\"ğŸ” æ£€æŸ¥å½“å‰æ˜¾å¡å ç”¨æƒ…å†µ:\")\n",
    "!rocm-smi --showpids\n",
    "\n",
    "# 2. å¼ºåˆ¶æ€æ‰æ‰€æœ‰ Python è¿›ç¨‹ (æ ¸å¼¹çº§æ¸…ç†)\n",
    "# æ³¨æ„ï¼šè¿™ä¼šé‡å¯å½“å‰çš„ Notebook å†…æ ¸ï¼Œä½ éœ€è¦é‡æ–°è¿è¡Œå‰é¢çš„ import å’Œ config ä»£ç \n",
    "print(\"ğŸ’£ æ­£åœ¨æ¸…ç†æ‰€æœ‰ Python è¿›ç¨‹...\")\n",
    "!pkill -9 python\n",
    "!pkill -9 python3\n",
    "\n",
    "print(\"âœ… æ¸…ç†å®Œæˆã€‚è¯·ç­‰å¾… 5 ç§’åï¼Œç‚¹å‡» Notebook ä¸Šæ–¹çš„ 'Restart Kernel'ï¼Œç„¶åä»å¤´è¿è¡Œé…ç½®ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8990774d-f322-453a-a16b-6268ad4e1e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ æ˜¾å­˜æ¸…ç†å®Œæ¯•\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"ğŸ§¹ æ˜¾å­˜æ¸…ç†å®Œæ¯•\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebffbb6-b43b-493c-9eba-8a312ea63124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# âœ… ä¿®æ­£ï¼šä½¿ç”¨å’Œä½ æˆªå›¾ä¸­å®Œå…¨ä¸€è‡´çš„æ–‡ä»¶å¤¹åç§°\n",
    "folder_name = \"LlamaFactory\" \n",
    "\n",
    "# æ£€æŸ¥å½“å‰ä½ç½®\n",
    "print(f\"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}\")\n",
    "\n",
    "if os.path.exists(folder_name):\n",
    "    os.chdir(\"/workspace/LlamaFactory\")\n",
    "    print(f\"ğŸ“‚ å·²åˆ‡æ¢å·¥ä½œç›®å½•è‡³: {os.getcwd()}\")\n",
    "    \n",
    "    # åˆ‡æ¢è¿›å»åå†è¿è¡Œå®‰è£…å‘½ä»¤\n",
    "    print(\"ğŸš€ å¼€å§‹å®‰è£…ä¾èµ–...\")\n",
    "    !pip install -e \".[metrics]\"\n",
    "    !pip install deepspeed\n",
    "    print(\"âœ… ç¯å¢ƒä¾èµ–å‡†å¤‡å°±ç»ª\")\n",
    "else:\n",
    "    print(f\"âŒ ä¾ç„¶æ‰¾ä¸åˆ°ç›®å½• '{folder_name}'\")\n",
    "    # æ‰“å°ä¸€ä¸‹å½“å‰ç›®å½•éƒ½æœ‰å•¥ï¼Œæ–¹ä¾¿è°ƒè¯•\n",
    "    print(f\"å½“å‰ç›®å½•ä¸‹çš„æ–‡ä»¶: {os.listdir('.')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8b30af-8882-4ef0-923e-d546d890dae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# ä½ çš„æ•°æ®æ–‡ä»¶å (è¯·ç¡®ä¿è¯¥æ–‡ä»¶åœ¨ /workspace ä¸‹ï¼Œæˆ–è€…å†™ç»å¯¹è·¯å¾„)\n",
    "# æ ¹æ®ä½ çš„æˆªå›¾ï¼Œæ•°æ®æ–‡ä»¶åº”è¯¥åœ¨ä¸Šä¸€çº§ç›®å½•\n",
    "DATA_FILE_PATH = \"/workspace/evol_instruct_dataset.json\" \n",
    "DATASET_NAME = \"my_evol_data\"\n",
    "\n",
    "# è¯»å–åŸæœ¬çš„ dataset_info.json\n",
    "info_file = \"data/dataset_info.json\"\n",
    "with open(info_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset_info = json.load(f)\n",
    "\n",
    "# æ·»åŠ ä½ çš„æ•°æ®é›†é…ç½®\n",
    "dataset_info[DATASET_NAME] = {\n",
    "    \"file_name\": DATA_FILE_PATH,\n",
    "    \"columns\": {\n",
    "        \"prompt\": \"instruction\",  # å¯¹åº” Distilabel ç”Ÿæˆçš„ instruction\n",
    "        \"response\": \"response\",   # å¯¹åº” Distilabel ç”Ÿæˆçš„ response\n",
    "        # \"system\": \"system\"      # å¦‚æœä½ çš„æ•°æ®é‡Œæ²¡æœ‰ system åˆ—ï¼Œè¿™è¡Œå¯ä»¥ä¸å†™\n",
    "    }\n",
    "}\n",
    "\n",
    "# å†™å›æ–‡ä»¶\n",
    "with open(info_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dataset_info, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"ğŸ‰ æ•°æ®é›† [{DATASET_NAME}] å·²æ³¨å†ŒæˆåŠŸï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9428c67-d588-434e-a844-cbca21a02486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ æ­£åœ¨æ‰§è¡Œæœ€åçš„æ˜¾å­˜æ£€æŸ¥ä¸æ¸…ç†...\n",
      "ğŸ“‰ å½“å‰æ˜¾å­˜å·²åˆ†é…: 0.00 GB\n",
      "ğŸ“‰ å½“å‰æ˜¾å­˜ä¿ç•™ä¸­: 0.00 GB\n",
      "âœ… å‡†å¤‡å°±ç»ª\n",
      "ğŸ“‚ å½“å‰å·¥ä½œç›®å½•: /workspace/LlamaFactory\n",
      "ğŸ“ æ»¡è¡€ç‰ˆé…ç½®æ–‡ä»¶å·²ç”Ÿæˆ: train_config_mi50_32g_full.yaml\n",
      "ğŸš€ å…¨é€Ÿå¯åŠ¨è®­ç»ƒ (Performance Mode)...\n",
      "   (è§‚å¯Ÿæ˜¾å­˜: å¦‚æœé•¿æ—¶é—´ä¸åŠ¨è¯·ä¸è¦æ…Œï¼Œæ­£åœ¨åˆ†é…å¤§å—æ˜¾å­˜)\n",
      "[INFO|2026-01-12 09:53:29] llamafactory.launcher:143 >> Initializing 1 distributed tasks at: 127.0.0.1:45739\n",
      "[WARNING|2026-01-12 09:53:42] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.\n",
      "[W112 09:53:46.472984849 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[INFO|2026-01-12 09:53:46] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.\n",
      "[INFO|2026-01-12 09:53:46] llamafactory.hparams.parser:465 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: True, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2093] 2026-01-12 09:53:46,201 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2093] 2026-01-12 09:53:46,201 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2093] 2026-01-12 09:53:46,201 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2093] 2026-01-12 09:53:46,201 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2093] 2026-01-12 09:53:46,201 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2093] 2026-01-12 09:53:46,201 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2093] 2026-01-12 09:53:46,201 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2364] 2026-01-12 09:53:46,384 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:763] 2026-01-12 09:53:46,384 >> loading configuration file /data/models/Qwen2.5-7B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:839] 2026-01-12 09:53:46,385 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"layer_types\": [\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2093] 2026-01-12 09:53:46,385 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2093] 2026-01-12 09:53:46,386 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2093] 2026-01-12 09:53:46,386 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2093] 2026-01-12 09:53:46,386 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2093] 2026-01-12 09:53:46,386 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2093] 2026-01-12 09:53:46,386 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2093] 2026-01-12 09:53:46,386 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2364] 2026-01-12 09:53:46,563 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2026-01-12 09:53:46] llamafactory.data.loader:143 >> Loading dataset /workspace/evol_instruct_dataset.json...\n",
      "num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n",
      "Converting format of dataset (num_proc=3): 100%|â–ˆ| 3/3 [00:00<00:00, 12.17 examp\n",
      "num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n",
      "Running tokenizer on dataset (num_proc=3): 100%|â–ˆ| 3/3 [00:00<00:00,  3.89 examp\n",
      "training example:\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 100007, 100134, 13027, 11319, 151645, 198, 151644, 77091, 198, 2, 13027, 53497, 99, 99347, 105866, 271, 565, 74866, 106, 23656, 198, 16, 13, 41479, 231, 98641, 13027, 10236, 236, 107, 99279, 198, 17, 13, 220, 50404, 106873, 13027, 64388, 230, 21894, 198, 18, 13, 53054, 100013, 99719, 198, 19, 13, 53497, 99, 99347, 99896, 117206, 198, 20, 13, 4891, 116, 116, 11622, 44956, 100157, 57218, 37029, 198, 21, 13, 92293, 99326, 73345, 100419, 198, 22, 13, 53497, 99, 99347, 103323, 57218, 86119, 100638, 271, 565, 220, 16, 13, 41479, 231, 98641, 13027, 10236, 236, 107, 99279, 271, 14374, 220, 16, 13, 16, 39095, 27366, 57218, 103999, 13027, 271, 101140, 112735, 62189, 13027, 1773, 104925, 13027, 41479, 246, 31139, 3703, 1110, 2136, 43193, 2659, 89185, 14, 74577, 114, 62189, 104027, 71109, 9370, 13027, 3407, 73594, 46216, 198, 2, 5515, 50042, 198, 12669, 12, 18, 1993, 1993, 32217, 67, 21, 19, 19399, 271, 2, 7401, 50042, 198, 30280, 12, 18, 1993, 1993, 1448, 41647, 87, 12, 16, 15, 13, 24, 83088, 271, 2, 14340, 50042, 198, 18881, 20095, 22491, 4582, 10135, 18, 198, 13874, 19324, 14374, 220, 16, 13, 17, 18137, 36677, 33477, 13027, 71951, 103999, 19108, 271, 104089, 106167, 22243, 102011, 3837, 31196, 1565, 12669, 1177, 4366, 63, 69353, 50930, 13027, 64388, 230, 21894, 27369, 3407, 73594, 46216, 198, 12669, 1177, 4366, 198, 13874, 19324, 565, 220, 17, 13, 220, 50404, 106873, 13027, 64388, 230, 21894, 271, 101898, 37029, 13027, 220, 18, 1993, 64388, 230, 21894, 3837, 99519, 13027, 220, 17, 1993, 83002, 110, 53393, 104390, 101999, 3407, 565, 220, 18, 13, 53054, 100013, 99719, 271, 14374, 220, 18, 13, 16, 85658, 30650, 6119, 271, 820, 220, 18, 13, 16, 13, 16, 41479, 231, 98641, 30650, 6119, 271, 104925, 3703, 1110, 1851, 52597, 59019, 905, 33630, 39095, 27366, 62926, 103999, 20185, 18563, 6119, 3407, 820, 220, 18, 13, 16, 13, 17, 41479, 231, 98641, 13027, 55059, 240, 14224, 271, 18493, 30650, 6119, 72858, 72651, 111687, 99600, 100910, 101047, 106375, 117556, 3837, 78973, 1565, 30280, 63, 74577, 114, 103999, 3407, 820, 220, 18, 13, 16, 13, 18, 41479, 231, 98641, 13027, 85122, 68862, 31548, 271, 104089, 104992, 3837, 104001, 87752, 106167, 103999, 1565, 12669, 43558, 26300, 63, 48443, 73594, 46216, 198, 51501, 4582, 10135, 43558, 26300, 58, 541, 921, 13874, 19324, 14374, 220, 18, 13, 17, 85658, 5355, 1143, 2178, 271, 820, 220, 18, 13, 17, 13, 16, 41479, 231, 98641, 5355, 1143, 2178, 271, 104925, 3703, 1110, 2136, 35510, 905, 90834, 98409, 33630, 14, 39095, 27366, 62926, 103999, 5355, 1143, 2178, 3407, 820, 220, 18, 13, 17, 13, 17, 18137, 44104, 21596, 13027, 85122, 68862, 31548, 271, 101159, 5355, 1143, 2178, 3837, 50377, 16628, 73345, 13343, 3837, 50404, 1565, 28560, 82493, 63, 3837, 101889, 42855, 56568, 36667, 103999, 9370, 13027, 85122, 68862, 31548, 76837, 3407, 565, 220, 19, 13, 53497, 99, 99347, 99896, 117206, 271, 14374, 220, 19, 13, 16, 26853, 246, 32757, 57218, 20074, 31905, 271, 73594, 12669, 198, 2, 43614, 112, 8863, 198, 64, 284, 220, 16, 15, 271, 2, 98313, 106, 27442, 8863, 198, 65, 284, 220, 16, 15, 13, 20, 271, 2, 73312, 38304, 51575, 198, 66, 284, 330, 9707, 11, 4337, 17199, 2, 4891, 116, 225, 99079, 25511, 198, 67, 284, 3007, 271, 2, 19468, 245, 20742, 198, 68, 284, 508, 16, 11, 220, 17, 11, 220, 18, 2533, 2, 34369, 225, 40027, 198, 69, 284, 320, 16, 11, 220, 17, 11, 220, 18, 692, 2, 73312, 99548, 198, 70, 284, 5212, 606, 788, 330, 48, 16948, 497, 330, 424, 788, 220, 17, 15, 532, 13874, 19324, 14374, 220, 19, 13, 17, 80851, 43316, 88653, 72881, 99700, 271, 820, 220, 19, 13, 17, 13, 16, 60596, 94, 14224, 72881, 99700, 271, 73594, 12669, 198, 333, 264, 861, 293, 510, 262, 1173, 445, 64, 40666, 100, 34204, 293, 1138, 12458, 264, 366, 293, 510, 262, 1173, 445, 64, 58230, 237, 34204, 293, 1138, 1503, 510, 262, 1173, 445, 64, 10236, 255, 231, 34204, 293, 1138, 13874, 19324, 820, 220, 19, 13, 17, 13, 17, 84141, 103, 86312, 271, 73594, 12669, 198, 1958, 600, 304, 2088, 7, 16, 15, 982, 262, 1173, 1956, 692, 3472, 264, 366, 220, 17, 15, 510, 262, 1173, 2877, 340, 262, 264, 1421, 220, 16, 198, 13874, 19324, 14374, 220, 19, 13, 18, 65727, 29941, 271, 73594, 12669, 198, 750, 912, 2877, 11, 293, 982, 262, 470, 264, 488, 293, 271, 1382, 284, 912, 7, 16, 15, 11, 220, 17, 15, 340, 1350, 4456, 340, 13874, 19324, 565, 220, 20, 13, 4891, 116, 116, 11622, 44956, 100157, 57218, 37029, 271, 14374, 220, 20, 13, 16, 16212, 13828, 271, 4651, 13828, 54851, 13027, 72858, 71817, 99891, 100768, 105549, 44956, 100653, 3407, 73594, 12669, 198, 474, 8591, 438, 2595, 271, 2, 47758, 69824, 198, 1118, 284, 2595, 7234, 2561, 16, 11, 220, 17, 11, 220, 18, 2546, 1350, 10939, 692, 2, 47685, 40027, 40090, 198, 1118, 17, 284, 2595, 7234, 2561, 19, 11, 220, 20, 11, 220, 21, 2546, 1350, 9900, 1364, 10939, 11, 2890, 17, 4390, 2, 44054, 253, 37643, 32804, 198, 1350, 9900, 18711, 10939, 1171, 13874, 19324, 14374, 220, 20, 13, 17, 33506, 300, 271, 47, 55433, 93685, 83744, 34187, 104795, 20074, 100166, 33108, 111540, 102011, 3407, 73594, 12669, 198, 474, 18617, 438, 7744, 271, 2, 47758, 45786, 198, 691, 284, 5360, 675, 1210, 2509, 24732, 516, 364, 30356, 516, 364, 13079, 4089, 364, 16749, 1210, 508, 17, 15, 11, 220, 17, 16, 11, 220, 16, 24, 23439, 2940, 284, 7744, 21077, 2592, 340, 1350, 16060, 692, 2, 62262, 54542, 198, 2940, 677, 16749, 660, 284, 6764, 677, 16749, 660, 488, 220, 16, 198, 1350, 16060, 340, 13874, 19324, 14374, 220, 20, 13, 18, 6867, 15406, 271, 11575, 15406, 54851, 13027, 43589, 100329, 28029, 44956, 3407, 73594, 12669, 198, 474, 16801, 23716, 438, 6516, 271, 2, 47758, 20074, 198, 87, 284, 508, 16, 11, 220, 17, 11, 220, 18, 11, 220, 19, 921, 88, 284, 508, 16, 15, 11, 220, 16, 20, 11, 220, 16, 18, 11, 220, 16, 22, 2533, 2, 44054, 246, 28029, 198, 9476, 12401, 2075, 11, 379, 340, 9476, 33098, 492, 55, 8908, 121, 112, 1305, 9476, 32962, 492, 56, 8908, 121, 112, 1305, 9476, 6067, 492, 19793, 26355, 116996, 1305, 151645, 198]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "å¦‚ä½•å­¦ä¹  Pythonï¼Ÿ<|im_end|>\n",
      "<|im_start|>assistant\n",
      "# Python å­¦ä¹ æŒ‡å—\n",
      "\n",
      "## ç›®å½•\n",
      "1. å®‰è£… Python ç¯å¢ƒ\n",
      "2. é€‰æ‹©åˆé€‚çš„ Python ç‰ˆæœ¬\n",
      "3. è®¾ç½®å¼€å‘ç¯å¢ƒ\n",
      "4. å­¦ä¹ åŸºç¡€è¯­æ³•\n",
      "5. å¸¸ç”¨åº“ä»‹ç»ä¸ä½¿ç”¨\n",
      "6. å®é™…é¡¹ç›®å®è·µ\n",
      "7. å­¦ä¹ å¿ƒå¾—ä¸é—®é¢˜è§£å†³\n",
      "\n",
      "## 1. å®‰è£… Python ç¯å¢ƒ\n",
      "\n",
      "### 1.1 ä¸‹è½½ä¸å®‰è£… Python\n",
      "\n",
      "é¦–å…ˆä½ éœ€è¦ä¸‹è½½ Pythonã€‚è®¿é—® Python å®˜ç½‘ https://www.python.org/downloads/ å¹¶ä¸‹è½½æœ€æ–°ç‰ˆæœ¬çš„ Pythonã€‚\n",
      "\n",
      "```bash\n",
      "# Windows ç”¨æˆ·\n",
      "python-3.x.x-amd64.exe\n",
      "\n",
      "# Mac ç”¨æˆ·\n",
      "Python-3.x.x-macosx-10.9.pkg\n",
      "\n",
      "# Linux ç”¨æˆ·\n",
      "sudo apt-get install python3\n",
      "```\n",
      "\n",
      "### 1.2 éªŒè¯ Python æ˜¯å¦å®‰è£…æˆåŠŸ\n",
      "\n",
      "æ‰“å¼€å‘½ä»¤è¡Œå·¥å…·ï¼Œè¾“å…¥ `python --version` æŸ¥çœ‹ Python ç‰ˆæœ¬ä¿¡æ¯ã€‚\n",
      "\n",
      "```bash\n",
      "python --version\n",
      "```\n",
      "\n",
      "## 2. é€‰æ‹©åˆé€‚çš„ Python ç‰ˆæœ¬\n",
      "\n",
      "å»ºè®®ä½¿ç”¨ Python 3.x ç‰ˆæœ¬ï¼Œå› ä¸º Python 2.x å·²ç»åœæ­¢ç»´æŠ¤ã€‚\n",
      "\n",
      "## 3. è®¾ç½®å¼€å‘ç¯å¢ƒ\n",
      "\n",
      "### 3.1 ä½¿ç”¨ VS Code\n",
      "\n",
      "#### 3.1.1 å®‰è£… VS Code\n",
      "\n",
      "è®¿é—® https://code.visualstudio.com/download ä¸‹è½½å¹¶å®‰è£… Visual Studio Codeã€‚\n",
      "\n",
      "#### 3.1.2 å®‰è£… Python æ’ä»¶\n",
      "\n",
      "åœ¨ VS Code ä¸­ç‚¹å‡»å·¦ä¾§æ´»åŠ¨æ ä¸­çš„æ‰©å±•å›¾æ ‡ï¼Œæœç´¢ `Python` å¹¶å®‰è£…ã€‚\n",
      "\n",
      "#### 3.1.3 å®‰è£… Python è§£é‡Šå™¨\n",
      "\n",
      "æ‰“å¼€ç»ˆç«¯ï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£… `python-language-server`ï¼š\n",
      "\n",
      "```bash\n",
      "pip install python-language-server[all]\n",
      "```\n",
      "\n",
      "### 3.2 ä½¿ç”¨ PyCharm\n",
      "\n",
      "#### 3.2.1 å®‰è£… PyCharm\n",
      "\n",
      "è®¿é—® https://www.jetbrains.com/pycharm/download/ ä¸‹è½½å¹¶å®‰è£… PyCharmã€‚\n",
      "\n",
      "#### 3.2.2 é…ç½® Python è§£é‡Šå™¨\n",
      "\n",
      "å¯åŠ¨ PyCharmï¼Œåˆ›å»ºæ–°é¡¹ç›®æ—¶ï¼Œé€‰æ‹© `Configure Interpreter`ï¼Œç„¶åæ·»åŠ ä½ å·²å®‰è£…çš„ Python è§£é‡Šå™¨è·¯å¾„ã€‚\n",
      "\n",
      "## 4. å­¦ä¹ åŸºç¡€è¯­æ³•\n",
      "\n",
      "### 4.1 å˜é‡ä¸æ•°æ®ç±»å‹\n",
      "\n",
      "```python\n",
      "# æ•´æ•°\n",
      "a = 10\n",
      "\n",
      "# æµ®ç‚¹æ•°\n",
      "b = 10.5\n",
      "\n",
      "# å­—ç¬¦ä¸²\n",
      "c = \"Hello, World!\"\n",
      "\n",
      "# å¸ƒå°”å€¼\n",
      "d = True\n",
      "\n",
      "# åˆ—è¡¨\n",
      "e = [1, 2, 3]\n",
      "\n",
      "# å…ƒç»„\n",
      "f = (1, 2, 3)\n",
      "\n",
      "# å­—å…¸\n",
      "g = {\"name\": \"Qwen\", \"age\": 20}\n",
      "```\n",
      "\n",
      "### 4.2 æ§åˆ¶æµè¯­å¥\n",
      "\n",
      "#### 4.2.1 æ¡ä»¶è¯­å¥\n",
      "\n",
      "```python\n",
      "if a > b:\n",
      "    print(\"a å¤§äº b\")\n",
      "elif a < b:\n",
      "    print(\"a å°äº b\")\n",
      "else:\n",
      "    print(\"a ç­‰äº b\")\n",
      "```\n",
      "\n",
      "#### 4.2.2 å¾ªç¯\n",
      "\n",
      "```python\n",
      "for i in range(10):\n",
      "    print(i)\n",
      "\n",
      "while a < 20:\n",
      "    print(a)\n",
      "    a += 1\n",
      "```\n",
      "\n",
      "### 4.3 å‡½æ•°\n",
      "\n",
      "```python\n",
      "def add(a, b):\n",
      "    return a + b\n",
      "\n",
      "result = add(10, 20)\n",
      "print(result)\n",
      "```\n",
      "\n",
      "## 5. å¸¸ç”¨åº“ä»‹ç»ä¸ä½¿ç”¨\n",
      "\n",
      "### 5.1 NumPy\n",
      "\n",
      "NumPy æ˜¯ Python ä¸­è¿›è¡Œç§‘å­¦è®¡ç®—çš„åŸºç¡€åº“ä¹‹ä¸€ã€‚\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "# åˆ›å»ºæ•°ç»„\n",
      "arr = np.array([1, 2, 3])\n",
      "print(arr)\n",
      "\n",
      "# æ•°ç»„æ“ä½œ\n",
      "arr2 = np.array([4, 5, 6])\n",
      "print(np.add(arr, arr2))\n",
      "\n",
      "# ç»Ÿè®¡å‡½æ•°\n",
      "print(np.mean(arr))\n",
      "```\n",
      "\n",
      "### 5.2 Pandas\n",
      "\n",
      "Pandas æä¾›äº†å¼ºå¤§çš„æ•°æ®ç»“æ„å’Œæ•°æ®åˆ†æå·¥å…·ã€‚\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# åˆ›å»º DataFrame\n",
      "data = {'Name': ['Tom', 'Nick', 'John'], 'Age': [20, 21, 19]}\n",
      "df = pd.DataFrame(data)\n",
      "print(df)\n",
      "\n",
      "# æ•°æ®å¤„ç†\n",
      "df['Age'] = df['Age'] + 1\n",
      "print(df)\n",
      "```\n",
      "\n",
      "### 5.3 Matplotlib\n",
      "\n",
      "Matplotlib æ˜¯ Python çš„ç»˜å›¾åº“ã€‚\n",
      "\n",
      "```python\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# åˆ›å»ºæ•°æ®\n",
      "x = [1, 2, 3, 4]\n",
      "y = [10, 15, 13, 17]\n",
      "\n",
      "# ç»˜å›¾\n",
      "plt.plot(x, y)\n",
      "plt.xlabel('X è½´')\n",
      "plt.ylabel('Y è½´')\n",
      "plt.title('ç¤ºä¾‹å›¾è¡¨')\n",
      "<|im_end|>\n",
      "\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2, 13027, 53497, 99, 99347, 105866, 271, 565, 74866, 106, 23656, 198, 16, 13, 41479, 231, 98641, 13027, 10236, 236, 107, 99279, 198, 17, 13, 220, 50404, 106873, 13027, 64388, 230, 21894, 198, 18, 13, 53054, 100013, 99719, 198, 19, 13, 53497, 99, 99347, 99896, 117206, 198, 20, 13, 4891, 116, 116, 11622, 44956, 100157, 57218, 37029, 198, 21, 13, 92293, 99326, 73345, 100419, 198, 22, 13, 53497, 99, 99347, 103323, 57218, 86119, 100638, 271, 565, 220, 16, 13, 41479, 231, 98641, 13027, 10236, 236, 107, 99279, 271, 14374, 220, 16, 13, 16, 39095, 27366, 57218, 103999, 13027, 271, 101140, 112735, 62189, 13027, 1773, 104925, 13027, 41479, 246, 31139, 3703, 1110, 2136, 43193, 2659, 89185, 14, 74577, 114, 62189, 104027, 71109, 9370, 13027, 3407, 73594, 46216, 198, 2, 5515, 50042, 198, 12669, 12, 18, 1993, 1993, 32217, 67, 21, 19, 19399, 271, 2, 7401, 50042, 198, 30280, 12, 18, 1993, 1993, 1448, 41647, 87, 12, 16, 15, 13, 24, 83088, 271, 2, 14340, 50042, 198, 18881, 20095, 22491, 4582, 10135, 18, 198, 13874, 19324, 14374, 220, 16, 13, 17, 18137, 36677, 33477, 13027, 71951, 103999, 19108, 271, 104089, 106167, 22243, 102011, 3837, 31196, 1565, 12669, 1177, 4366, 63, 69353, 50930, 13027, 64388, 230, 21894, 27369, 3407, 73594, 46216, 198, 12669, 1177, 4366, 198, 13874, 19324, 565, 220, 17, 13, 220, 50404, 106873, 13027, 64388, 230, 21894, 271, 101898, 37029, 13027, 220, 18, 1993, 64388, 230, 21894, 3837, 99519, 13027, 220, 17, 1993, 83002, 110, 53393, 104390, 101999, 3407, 565, 220, 18, 13, 53054, 100013, 99719, 271, 14374, 220, 18, 13, 16, 85658, 30650, 6119, 271, 820, 220, 18, 13, 16, 13, 16, 41479, 231, 98641, 30650, 6119, 271, 104925, 3703, 1110, 1851, 52597, 59019, 905, 33630, 39095, 27366, 62926, 103999, 20185, 18563, 6119, 3407, 820, 220, 18, 13, 16, 13, 17, 41479, 231, 98641, 13027, 55059, 240, 14224, 271, 18493, 30650, 6119, 72858, 72651, 111687, 99600, 100910, 101047, 106375, 117556, 3837, 78973, 1565, 30280, 63, 74577, 114, 103999, 3407, 820, 220, 18, 13, 16, 13, 18, 41479, 231, 98641, 13027, 85122, 68862, 31548, 271, 104089, 104992, 3837, 104001, 87752, 106167, 103999, 1565, 12669, 43558, 26300, 63, 48443, 73594, 46216, 198, 51501, 4582, 10135, 43558, 26300, 58, 541, 921, 13874, 19324, 14374, 220, 18, 13, 17, 85658, 5355, 1143, 2178, 271, 820, 220, 18, 13, 17, 13, 16, 41479, 231, 98641, 5355, 1143, 2178, 271, 104925, 3703, 1110, 2136, 35510, 905, 90834, 98409, 33630, 14, 39095, 27366, 62926, 103999, 5355, 1143, 2178, 3407, 820, 220, 18, 13, 17, 13, 17, 18137, 44104, 21596, 13027, 85122, 68862, 31548, 271, 101159, 5355, 1143, 2178, 3837, 50377, 16628, 73345, 13343, 3837, 50404, 1565, 28560, 82493, 63, 3837, 101889, 42855, 56568, 36667, 103999, 9370, 13027, 85122, 68862, 31548, 76837, 3407, 565, 220, 19, 13, 53497, 99, 99347, 99896, 117206, 271, 14374, 220, 19, 13, 16, 26853, 246, 32757, 57218, 20074, 31905, 271, 73594, 12669, 198, 2, 43614, 112, 8863, 198, 64, 284, 220, 16, 15, 271, 2, 98313, 106, 27442, 8863, 198, 65, 284, 220, 16, 15, 13, 20, 271, 2, 73312, 38304, 51575, 198, 66, 284, 330, 9707, 11, 4337, 17199, 2, 4891, 116, 225, 99079, 25511, 198, 67, 284, 3007, 271, 2, 19468, 245, 20742, 198, 68, 284, 508, 16, 11, 220, 17, 11, 220, 18, 2533, 2, 34369, 225, 40027, 198, 69, 284, 320, 16, 11, 220, 17, 11, 220, 18, 692, 2, 73312, 99548, 198, 70, 284, 5212, 606, 788, 330, 48, 16948, 497, 330, 424, 788, 220, 17, 15, 532, 13874, 19324, 14374, 220, 19, 13, 17, 80851, 43316, 88653, 72881, 99700, 271, 820, 220, 19, 13, 17, 13, 16, 60596, 94, 14224, 72881, 99700, 271, 73594, 12669, 198, 333, 264, 861, 293, 510, 262, 1173, 445, 64, 40666, 100, 34204, 293, 1138, 12458, 264, 366, 293, 510, 262, 1173, 445, 64, 58230, 237, 34204, 293, 1138, 1503, 510, 262, 1173, 445, 64, 10236, 255, 231, 34204, 293, 1138, 13874, 19324, 820, 220, 19, 13, 17, 13, 17, 84141, 103, 86312, 271, 73594, 12669, 198, 1958, 600, 304, 2088, 7, 16, 15, 982, 262, 1173, 1956, 692, 3472, 264, 366, 220, 17, 15, 510, 262, 1173, 2877, 340, 262, 264, 1421, 220, 16, 198, 13874, 19324, 14374, 220, 19, 13, 18, 65727, 29941, 271, 73594, 12669, 198, 750, 912, 2877, 11, 293, 982, 262, 470, 264, 488, 293, 271, 1382, 284, 912, 7, 16, 15, 11, 220, 17, 15, 340, 1350, 4456, 340, 13874, 19324, 565, 220, 20, 13, 4891, 116, 116, 11622, 44956, 100157, 57218, 37029, 271, 14374, 220, 20, 13, 16, 16212, 13828, 271, 4651, 13828, 54851, 13027, 72858, 71817, 99891, 100768, 105549, 44956, 100653, 3407, 73594, 12669, 198, 474, 8591, 438, 2595, 271, 2, 47758, 69824, 198, 1118, 284, 2595, 7234, 2561, 16, 11, 220, 17, 11, 220, 18, 2546, 1350, 10939, 692, 2, 47685, 40027, 40090, 198, 1118, 17, 284, 2595, 7234, 2561, 19, 11, 220, 20, 11, 220, 21, 2546, 1350, 9900, 1364, 10939, 11, 2890, 17, 4390, 2, 44054, 253, 37643, 32804, 198, 1350, 9900, 18711, 10939, 1171, 13874, 19324, 14374, 220, 20, 13, 17, 33506, 300, 271, 47, 55433, 93685, 83744, 34187, 104795, 20074, 100166, 33108, 111540, 102011, 3407, 73594, 12669, 198, 474, 18617, 438, 7744, 271, 2, 47758, 45786, 198, 691, 284, 5360, 675, 1210, 2509, 24732, 516, 364, 30356, 516, 364, 13079, 4089, 364, 16749, 1210, 508, 17, 15, 11, 220, 17, 16, 11, 220, 16, 24, 23439, 2940, 284, 7744, 21077, 2592, 340, 1350, 16060, 692, 2, 62262, 54542, 198, 2940, 677, 16749, 660, 284, 6764, 677, 16749, 660, 488, 220, 16, 198, 1350, 16060, 340, 13874, 19324, 14374, 220, 20, 13, 18, 6867, 15406, 271, 11575, 15406, 54851, 13027, 43589, 100329, 28029, 44956, 3407, 73594, 12669, 198, 474, 16801, 23716, 438, 6516, 271, 2, 47758, 20074, 198, 87, 284, 508, 16, 11, 220, 17, 11, 220, 18, 11, 220, 19, 921, 88, 284, 508, 16, 15, 11, 220, 16, 20, 11, 220, 16, 18, 11, 220, 16, 22, 2533, 2, 44054, 246, 28029, 198, 9476, 12401, 2075, 11, 379, 340, 9476, 33098, 492, 55, 8908, 121, 112, 1305, 9476, 32962, 492, 56, 8908, 121, 112, 1305, 9476, 6067, 492, 19793, 26355, 116996, 1305, 151645, 198]\n",
      "labels:\n",
      "# Python å­¦ä¹ æŒ‡å—\n",
      "\n",
      "## ç›®å½•\n",
      "1. å®‰è£… Python ç¯å¢ƒ\n",
      "2. é€‰æ‹©åˆé€‚çš„ Python ç‰ˆæœ¬\n",
      "3. è®¾ç½®å¼€å‘ç¯å¢ƒ\n",
      "4. å­¦ä¹ åŸºç¡€è¯­æ³•\n",
      "5. å¸¸ç”¨åº“ä»‹ç»ä¸ä½¿ç”¨\n",
      "6. å®é™…é¡¹ç›®å®è·µ\n",
      "7. å­¦ä¹ å¿ƒå¾—ä¸é—®é¢˜è§£å†³\n",
      "\n",
      "## 1. å®‰è£… Python ç¯å¢ƒ\n",
      "\n",
      "### 1.1 ä¸‹è½½ä¸å®‰è£… Python\n",
      "\n",
      "é¦–å…ˆä½ éœ€è¦ä¸‹è½½ Pythonã€‚è®¿é—® Python å®˜ç½‘ https://www.python.org/downloads/ å¹¶ä¸‹è½½æœ€æ–°ç‰ˆæœ¬çš„ Pythonã€‚\n",
      "\n",
      "```bash\n",
      "# Windows ç”¨æˆ·\n",
      "python-3.x.x-amd64.exe\n",
      "\n",
      "# Mac ç”¨æˆ·\n",
      "Python-3.x.x-macosx-10.9.pkg\n",
      "\n",
      "# Linux ç”¨æˆ·\n",
      "sudo apt-get install python3\n",
      "```\n",
      "\n",
      "### 1.2 éªŒè¯ Python æ˜¯å¦å®‰è£…æˆåŠŸ\n",
      "\n",
      "æ‰“å¼€å‘½ä»¤è¡Œå·¥å…·ï¼Œè¾“å…¥ `python --version` æŸ¥çœ‹ Python ç‰ˆæœ¬ä¿¡æ¯ã€‚\n",
      "\n",
      "```bash\n",
      "python --version\n",
      "```\n",
      "\n",
      "## 2. é€‰æ‹©åˆé€‚çš„ Python ç‰ˆæœ¬\n",
      "\n",
      "å»ºè®®ä½¿ç”¨ Python 3.x ç‰ˆæœ¬ï¼Œå› ä¸º Python 2.x å·²ç»åœæ­¢ç»´æŠ¤ã€‚\n",
      "\n",
      "## 3. è®¾ç½®å¼€å‘ç¯å¢ƒ\n",
      "\n",
      "### 3.1 ä½¿ç”¨ VS Code\n",
      "\n",
      "#### 3.1.1 å®‰è£… VS Code\n",
      "\n",
      "è®¿é—® https://code.visualstudio.com/download ä¸‹è½½å¹¶å®‰è£… Visual Studio Codeã€‚\n",
      "\n",
      "#### 3.1.2 å®‰è£… Python æ’ä»¶\n",
      "\n",
      "åœ¨ VS Code ä¸­ç‚¹å‡»å·¦ä¾§æ´»åŠ¨æ ä¸­çš„æ‰©å±•å›¾æ ‡ï¼Œæœç´¢ `Python` å¹¶å®‰è£…ã€‚\n",
      "\n",
      "#### 3.1.3 å®‰è£… Python è§£é‡Šå™¨\n",
      "\n",
      "æ‰“å¼€ç»ˆç«¯ï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£… `python-language-server`ï¼š\n",
      "\n",
      "```bash\n",
      "pip install python-language-server[all]\n",
      "```\n",
      "\n",
      "### 3.2 ä½¿ç”¨ PyCharm\n",
      "\n",
      "#### 3.2.1 å®‰è£… PyCharm\n",
      "\n",
      "è®¿é—® https://www.jetbrains.com/pycharm/download/ ä¸‹è½½å¹¶å®‰è£… PyCharmã€‚\n",
      "\n",
      "#### 3.2.2 é…ç½® Python è§£é‡Šå™¨\n",
      "\n",
      "å¯åŠ¨ PyCharmï¼Œåˆ›å»ºæ–°é¡¹ç›®æ—¶ï¼Œé€‰æ‹© `Configure Interpreter`ï¼Œç„¶åæ·»åŠ ä½ å·²å®‰è£…çš„ Python è§£é‡Šå™¨è·¯å¾„ã€‚\n",
      "\n",
      "## 4. å­¦ä¹ åŸºç¡€è¯­æ³•\n",
      "\n",
      "### 4.1 å˜é‡ä¸æ•°æ®ç±»å‹\n",
      "\n",
      "```python\n",
      "# æ•´æ•°\n",
      "a = 10\n",
      "\n",
      "# æµ®ç‚¹æ•°\n",
      "b = 10.5\n",
      "\n",
      "# å­—ç¬¦ä¸²\n",
      "c = \"Hello, World!\"\n",
      "\n",
      "# å¸ƒå°”å€¼\n",
      "d = True\n",
      "\n",
      "# åˆ—è¡¨\n",
      "e = [1, 2, 3]\n",
      "\n",
      "# å…ƒç»„\n",
      "f = (1, 2, 3)\n",
      "\n",
      "# å­—å…¸\n",
      "g = {\"name\": \"Qwen\", \"age\": 20}\n",
      "```\n",
      "\n",
      "### 4.2 æ§åˆ¶æµè¯­å¥\n",
      "\n",
      "#### 4.2.1 æ¡ä»¶è¯­å¥\n",
      "\n",
      "```python\n",
      "if a > b:\n",
      "    print(\"a å¤§äº b\")\n",
      "elif a < b:\n",
      "    print(\"a å°äº b\")\n",
      "else:\n",
      "    print(\"a ç­‰äº b\")\n",
      "```\n",
      "\n",
      "#### 4.2.2 å¾ªç¯\n",
      "\n",
      "```python\n",
      "for i in range(10):\n",
      "    print(i)\n",
      "\n",
      "while a < 20:\n",
      "    print(a)\n",
      "    a += 1\n",
      "```\n",
      "\n",
      "### 4.3 å‡½æ•°\n",
      "\n",
      "```python\n",
      "def add(a, b):\n",
      "    return a + b\n",
      "\n",
      "result = add(10, 20)\n",
      "print(result)\n",
      "```\n",
      "\n",
      "## 5. å¸¸ç”¨åº“ä»‹ç»ä¸ä½¿ç”¨\n",
      "\n",
      "### 5.1 NumPy\n",
      "\n",
      "NumPy æ˜¯ Python ä¸­è¿›è¡Œç§‘å­¦è®¡ç®—çš„åŸºç¡€åº“ä¹‹ä¸€ã€‚\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "# åˆ›å»ºæ•°ç»„\n",
      "arr = np.array([1, 2, 3])\n",
      "print(arr)\n",
      "\n",
      "# æ•°ç»„æ“ä½œ\n",
      "arr2 = np.array([4, 5, 6])\n",
      "print(np.add(arr, arr2))\n",
      "\n",
      "# ç»Ÿè®¡å‡½æ•°\n",
      "print(np.mean(arr))\n",
      "```\n",
      "\n",
      "### 5.2 Pandas\n",
      "\n",
      "Pandas æä¾›äº†å¼ºå¤§çš„æ•°æ®ç»“æ„å’Œæ•°æ®åˆ†æå·¥å…·ã€‚\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# åˆ›å»º DataFrame\n",
      "data = {'Name': ['Tom', 'Nick', 'John'], 'Age': [20, 21, 19]}\n",
      "df = pd.DataFrame(data)\n",
      "print(df)\n",
      "\n",
      "# æ•°æ®å¤„ç†\n",
      "df['Age'] = df['Age'] + 1\n",
      "print(df)\n",
      "```\n",
      "\n",
      "### 5.3 Matplotlib\n",
      "\n",
      "Matplotlib æ˜¯ Python çš„ç»˜å›¾åº“ã€‚\n",
      "\n",
      "```python\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# åˆ›å»ºæ•°æ®\n",
      "x = [1, 2, 3, 4]\n",
      "y = [10, 15, 13, 17]\n",
      "\n",
      "# ç»˜å›¾\n",
      "plt.plot(x, y)\n",
      "plt.xlabel('X è½´')\n",
      "plt.ylabel('Y è½´')\n",
      "plt.title('ç¤ºä¾‹å›¾è¡¨')\n",
      "<|im_end|>\n",
      "\n",
      "[INFO|configuration_utils.py:763] 2026-01-12 09:53:48,661 >> loading configuration file /data/models/Qwen2.5-7B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:839] 2026-01-12 09:53:48,662 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"layer_types\": [\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "[INFO|2026-01-12 09:53:48] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
      "[WARNING|logging.py:328] 2026-01-12 09:53:48,888 >> `torch_dtype` is deprecated! Use `dtype` instead!\n",
      "[INFO|modeling_utils.py:1169] 2026-01-12 09:53:48,888 >> loading weights file /data/models/Qwen2.5-7B-Instruct/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1243] 2026-01-12 09:53:48,888 >> Will use dtype=torch.bfloat16 as defined in model's config object\n",
      "[INFO|modeling_utils.py:2341] 2026-01-12 09:53:48,889 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:986] 2026-01-12 09:53:48,890 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /build/pytorch/c10/hip/HIPAllocatorConfig.h:36.)\n",
      "  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:07<00:00,  1.89s/it]\n",
      "[INFO|configuration_utils.py:939] 2026-01-12 09:53:56,470 >> loading configuration file /data/models/Qwen2.5-7B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:986] 2026-01-12 09:53:56,470 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.05,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "[INFO|dynamic_module_utils.py:423] 2026-01-12 09:53:56,470 >> Could not locate the custom_generate/generate.py inside /data/models/Qwen2.5-7B-Instruct.\n",
      "[INFO|2026-01-12 09:53:56] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
      "[INFO|2026-01-12 09:53:56] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2026-01-12 09:53:56] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
      "[INFO|2026-01-12 09:53:56] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
      "[INFO|2026-01-12 09:53:56] llamafactory.model.model_utils.misc:143 >> Found linear modules: gate_proj,up_proj,down_proj,o_proj,k_proj,v_proj,q_proj\n",
      "[INFO|2026-01-12 09:53:57] llamafactory.model.loader:143 >> trainable params: 80,740,352 || all params: 7,696,356,864 || trainable%: 1.0491\n",
      "[WARNING|trainer.py:906] 2026-01-12 09:53:57,190 >> The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "[INFO|trainer.py:749] 2026-01-12 09:53:58,980 >> Using auto half precision backend\n",
      "[WARNING|trainer.py:982] 2026-01-12 09:53:58,981 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "[INFO|trainer.py:2519] 2026-01-12 09:53:59,417 >> ***** Running training *****\n",
      "[INFO|trainer.py:2520] 2026-01-12 09:53:59,417 >>   Num examples = 3\n",
      "[INFO|trainer.py:2521] 2026-01-12 09:53:59,417 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:2522] 2026-01-12 09:53:59,417 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:2525] 2026-01-12 09:53:59,417 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:2526] 2026-01-12 09:53:59,417 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2527] 2026-01-12 09:53:59,417 >>   Total optimization steps = 10\n",
      "[INFO|trainer.py:2528] 2026-01-12 09:53:59,419 >>   Number of trainable parameters = 80,740,352\n",
      "{'loss': 0.4228, 'grad_norm': 0.1795356720685959, 'learning_rate': 7.500000000000001e-05, 'epoch': 5.0}\n",
      "{'loss': 0.2752, 'grad_norm': 0.15594929456710815, 'learning_rate': 3.0153689607045845e-06, 'epoch': 10.0}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:06<00:00, 12.89s/it][INFO|trainer.py:4309] 2026-01-12 09:56:06,089 >> Saving model checkpoint to /workspace/saves/qwen2.5-lora-32g-full/checkpoint-10\n",
      "[INFO|configuration_utils.py:763] 2026-01-12 09:56:06,103 >> loading configuration file /data/models/Qwen2.5-7B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:839] 2026-01-12 09:56:06,104 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"layer_types\": [\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2421] 2026-01-12 09:56:06,512 >> chat template saved in /workspace/saves/qwen2.5-lora-32g-full/checkpoint-10/chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2590] 2026-01-12 09:56:06,513 >> tokenizer config file saved in /workspace/saves/qwen2.5-lora-32g-full/checkpoint-10/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2599] 2026-01-12 09:56:06,513 >> Special tokens file saved in /workspace/saves/qwen2.5-lora-32g-full/checkpoint-10/special_tokens_map.json\n",
      "[INFO|trainer.py:2810] 2026-01-12 09:56:07,211 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 127.7926, 'train_samples_per_second': 0.235, 'train_steps_per_second': 0.078, 'train_loss': 0.34896589517593385, 'epoch': 10.0}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:07<00:00, 12.78s/it]\n",
      "[INFO|trainer.py:4309] 2026-01-12 09:56:07,214 >> Saving model checkpoint to /workspace/saves/qwen2.5-lora-32g-full\n",
      "[INFO|configuration_utils.py:763] 2026-01-12 09:56:07,229 >> loading configuration file /data/models/Qwen2.5-7B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:839] 2026-01-12 09:56:07,229 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"layer_types\": [\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2421] 2026-01-12 09:56:07,634 >> chat template saved in /workspace/saves/qwen2.5-lora-32g-full/chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2590] 2026-01-12 09:56:07,634 >> tokenizer config file saved in /workspace/saves/qwen2.5-lora-32g-full/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2599] 2026-01-12 09:56:07,635 >> Special tokens file saved in /workspace/saves/qwen2.5-lora-32g-full/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =       10.0\n",
      "  total_flos               =  1275565GF\n",
      "  train_loss               =      0.349\n",
      "  train_runtime            = 0:02:07.79\n",
      "  train_samples_per_second =      0.235\n",
      "  train_steps_per_second   =      0.078\n",
      "Figure saved at: /workspace/saves/qwen2.5-lora-32g-full/training_loss.png\n",
      "[WARNING|2026-01-12 09:56:07] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.\n",
      "[WARNING|2026-01-12 09:56:07] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.\n",
      "[INFO|modelcard.py:456] 2026-01-12 09:56:07,835 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# ================= ğŸ§¹ 1. å†æ¬¡ç¡®ä¿¡æ˜¾å­˜æ˜¯ç©ºçš„ =================\n",
    "print(\"ğŸ§¹ æ­£åœ¨æ‰§è¡Œæœ€åçš„æ˜¾å­˜æ£€æŸ¥ä¸æ¸…ç†...\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    # æ‰“å°å½“å‰æ˜¾å­˜å ç”¨ï¼Œç¡®ä¿æ˜¯çœŸçš„ç©ºäº† (åº”è¯¥åœ¨ 1GB ä»¥å†…)\n",
    "    print(f\"ğŸ“‰ å½“å‰æ˜¾å­˜å·²åˆ†é…: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"ğŸ“‰ å½“å‰æ˜¾å­˜ä¿ç•™ä¸­: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "print(\"âœ… å‡†å¤‡å°±ç»ª\")\n",
    "\n",
    "# ç¡®ä¿åœ¨æ­£ç¡®çš„å·¥ä½œç›®å½•\n",
    "if os.path.exists(\"LlamaFactory\"):\n",
    "    os.chdir(\"LlamaFactory\")\n",
    "print(f\"ğŸ“‚ å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}\")\n",
    "\n",
    "# ================= âš™ï¸ 2. ç”Ÿæˆ 32GB æ»¡è¡€ç‰ˆé…ç½® =================\n",
    "config = {\n",
    "    # --- æ¨¡å‹ä¸æ•°æ® ---\n",
    "    \"model_name_or_path\": \"/data/models/Qwen2.5-7B-Instruct\",\n",
    "    \"dataset\": \"my_evol_data\",\n",
    "    \"template\": \"qwen\",\n",
    "    \"overwrite_cache\": True,\n",
    "    \"preprocessing_num_workers\": 16,\n",
    "\n",
    "    # --- è¾“å‡ºé…ç½® ---\n",
    "    \"output_dir\": \"/workspace/saves/qwen2.5-lora-32g-full\",\n",
    "    \"logging_steps\": 5,          # æ­£å¸¸æ­¥æ•°è®°å½•\n",
    "    \"save_steps\": 50,            # æ¯ 50 æ­¥å­˜æ¡£\n",
    "    \"plot_loss\": True,\n",
    "    \"overwrite_output_dir\": True,\n",
    "\n",
    "    # --- è®­ç»ƒæ ¸å¿ƒ ---\n",
    "    \"stage\": \"sft\",\n",
    "    \"do_train\": True,\n",
    "    \"finetuning_type\": \"lora\",\n",
    "    \"lora_target\": \"all\",\n",
    "    \n",
    "    # ğŸ”¥ æ€§èƒ½å‡çº§ç‚¹ A: Rank ç¿»å€ï¼Œæ›´èªæ˜çš„å¾®è°ƒ\n",
    "    \"lora_rank\": 32,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "\n",
    "    # --- ç¡¬ä»¶ä¼˜åŒ– (32GB æ»¡è¡€æ¨¡å¼) ---\n",
    "    \n",
    "    # ğŸ”¥ æ€§èƒ½å‡çº§ç‚¹ B: è§£é” 4k é•¿ä¸Šä¸‹æ–‡\n",
    "    \"cutoff_len\": 4096,\n",
    "    \n",
    "    # ğŸ”¥ æ€§èƒ½å‡çº§ç‚¹ C: Batch Size ç¿» 4 å€ (ååé‡å¤§å¢)\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \n",
    "    # ğŸ”¥ é…åˆè°ƒæ•´: 4(BS) * 4(Accum) = 16 (æ€»æœ‰æ•ˆ Batch)\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \n",
    "    \"learning_rate\": 1.0e-4,\n",
    "    \"num_train_epochs\": 10.0,    # è®¾ä¸º 10 è½®ï¼Œç¡®ä¿å³ä½¿æ•°æ®å°‘ä¹Ÿèƒ½è·‘ä¸€ä¼šå„¿\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \n",
    "    # âš¡ MI50 æ ¸å¿ƒé“å¾‹: å¿…é¡» FP16ï¼Œä¸¥ç¦ BF16\n",
    "    \"fp16\": True,\n",
    "    \n",
    "    \"ddp_timeout\": 18000000,\n",
    "    # åªæœ‰2æ¡æ•°æ®æ—¶å»ºè®®å…³é—­éªŒè¯ï¼Œé˜²æ­¢æŠ¥é”™ï¼›æ•°æ®å¤šæ—¶å¯å¼€å¯\n",
    "    \"val_size\": 0.0, \n",
    "    # \"eval_strategy\": \"steps\",\n",
    "    # \"eval_steps\": 100,\n",
    "    # \"per_device_eval_batch_size\": 4,\n",
    "}\n",
    "\n",
    "# ä¿å­˜é…ç½®æ–‡ä»¶\n",
    "config_file = \"train_config_mi50_32g_full.yaml\"\n",
    "with open(config_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.dump(config, f, sort_keys=False)\n",
    "\n",
    "print(f\"ğŸ“ æ»¡è¡€ç‰ˆé…ç½®æ–‡ä»¶å·²ç”Ÿæˆ: {config_file}\")\n",
    "\n",
    "# ================= ğŸš€ 3. å¼€å§‹è®­ç»ƒ =================\n",
    "print(\"ğŸš€ å…¨é€Ÿå¯åŠ¨è®­ç»ƒ (Performance Mode)...\")\n",
    "print(\"   (è§‚å¯Ÿæ˜¾å­˜: å¦‚æœé•¿æ—¶é—´ä¸åŠ¨è¯·ä¸è¦æ…Œï¼Œæ­£åœ¨åˆ†é…å¤§å—æ˜¾å­˜)\")\n",
    "\n",
    "!FORCE_TORCHRUN=1 llamafactory-cli train train_config_mi50_32g_full.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ba40ea-8897-4048-9182-71f447db7d30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
